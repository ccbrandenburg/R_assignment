---
title: "R Assignment"
output: html_document
---

Outline (to be deleted following completion):

1. Introduction to project
+ purpose
+ data
+ goal

2. Data exploration
+ description
+ explanation of data retained
+ data cleaning

3.Analyses
- Summary
- 1-D plots
- 2-D plots
- 3-D plots
- Correlations
- Regressions
- Validation

Summary
- results
- techniques used
- what was missing/failed
- conclusion (lessons learned)



INTRODUCTION

The purpose of this project was to explore a data set using R techniques learned during the course, particularly from the dplyr and ggplot2 libraries. The data set we selected contains information on nearly 40,000 articles published by Mashable over a 2-year period, detailing the day of publication, number of shares, topic of the content and keyword information. Our goal was to identify a meaningful relationship between the articles and their reach (as measured in shares).


DATA CLEANING

The initial data set, downloaded from the University of California Irvine Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity), had 39644 observations and 61 variables.

Loading required libraries
```{r library}
library(ggplot2)
library(dplyr)
library(GGally)
library(car)
library(gridExtra)
```

Loading and examining the data
```{r load data and examine}
data=read.csv("OnlineNewsPopularity.csv", stringsAsFactors = FALSE)
glimpse(data)
dim(data)
tbl_df(data)
head(data)

#PCA
log.data = log

# set the seed to make results reproducible
#set.seed(27)
#data.subset <- data[, c(2:15)]
#ggpairs(data.subset[sample.int(nrow(data.subset), 100), ])
```

To start, we selected only those columns that we believed had a higher probability of correlating positively with increased shares, such as type of content (images, links, videos), length of content (word count in title or article body), day of week published, topic of article, subjectivity and sentiment analysis information. The variables we removed either had unclear descriptions, were deemed redundant given other variables provided, or were not interesting enough to pursue in our analysis.

```{r variable-reduction}
data_s = data %>%
  select(url,
         timedelta,
         n_tokens_title,
         n_tokens_content,
  # removed variables 4-6
         num_hrefs,
         num_self_hrefs,
         num_imgs,
         num_videos,
         average_token_length,
         num_keywords,
         data_channel_is_lifestyle,
         data_channel_is_entertainment,
         data_channel_is_bus,
         data_channel_is_socmed,
         data_channel_is_tech,
         data_channel_is_world,
  # removed variables 19-30
         weekday_is_monday, 
         weekday_is_tuesday, 
         weekday_is_wednesday,
         weekday_is_thursday, 
         weekday_is_friday,
         weekday_is_saturday,
         weekday_is_sunday,
         is_weekend,
  # removed variables 39-43
         global_subjectivity,
         global_sentiment_polarity,
         global_rate_positive_words,
         global_rate_negative_words,
         rate_positive_words,
         rate_negative_words,
  # removed variables 50-55
         title_subjectivity,
         title_sentiment_polarity,
         abs_title_subjectivity,
         abs_title_sentiment_polarity,
         shares)
```

To make the data easier to manage, we mutated the flag variables that could be easily consolidated into one categorical variable. For example, instead of a categorical variable citing which day of the week the article was published, the original data set had a flag variable for each day of the week stating whether or not the article had been published that day. This was also the case for data_channel, which describes the topic of the article.

There existed a flag variable called "is_weekend" that marks articles that were published on the weekend. We wanted to explore this, but the definition of weekend was not included. Rather than assume this meant Saturday and Sunday, we created our own flag variable called weekday, which represented any articles published Monday through Friday.

```{r mutating-variables}
data_s = data_s %>%
  #create weekday variable 
  mutate(weekday = ifelse(weekday_is_monday==1, 1,
                    ifelse(weekday_is_tuesday==1, 2,
                    ifelse(weekday_is_wednesday==1, 3,
                    ifelse(weekday_is_thursday==1, 4,
                    ifelse(weekday_is_friday==1, 5,
                    ifelse(weekday_is_saturday==1, 6,
                    ifelse(weekday_is_sunday==1, 7,0)))))))) %>%
  mutate(weekday_flag = ifelse(weekday<=5, 1, 0)) %>%
  #create channel variable
  mutate(data_channel = ifelse(data_channel_is_entertainment==1, "Entertainment",
                        ifelse(data_channel_is_bus==1, "Business",
                        ifelse(data_channel_is_socmed==1, "Social Media",
                        ifelse(data_channel_is_tech==1, "Tech",
                        ifelse(data_channel_is_world==1, "World",
                        ifelse(data_channel_is_lifestyle==1, "Lifestyle", "Other")))))))
```

DATA EXPLORATION

Our initial exploration of the data was focused on summarizing and visualizing the variables individually, and then plotting shares vs. various independent variables.


@Lionel and @Iyad, I think we can delete the following chunk. I took out the visualisation of positive keywords and added negative plus two correlations in the chunk below. 
```{r}
# should we go in order here:
# 1. summarize the statistics & point out interesting tidbits
# 2. plot individual variables
# 3. plot individual variables vs. shares

#plotting timedelta on a histogram
qplot(data=data_s, 
      timedelta, 
      geom="histogram") +
  theme_grey()


#plotting n_tokens_title on a histogram
qplot(data=data_s,
      n_tokens_title,
      geom="histogram") +
  theme_grey()

#what are we doing here?
log.data = log

qplot(data=data_s,
      title_sentiment_polarity,
      geom="histogram")   

qplot(data=data_s,
      title_sentiment_polarity,
      shares)

summary(data_s$shares)


```

The following graphs are used to understand whether there are visual differences between how positive and negative words affect the number of shares. From the graphs and the correlations there does not seem to be a relationship. (Not sure if it makes sense to do this analysis at all or where in the project we could place this)
```{r negative vs positive words}
qpos = qplot(data=data_s,
      rate_positive_words,
      shares,
      log="xy") +
  geom_point(alpha=1/200)

qneg = qplot(data=data_s,
      rate_negative_words,
      shares,
      log="xy") +
  geom_point(alpha=1/200)

grid.arrange(qpos, qneg, nrow=1)

cor(data_s$rate_positive_words, data_s$shares)

cor(data_s$rate_negative_words, data_s$shares)
```

CHANNEL ANALYSIS
The following section will investigate our data at the channel level in order to see whether there are channels/topics that stand out.

Basic statistical overview of shares per channel:
```{r cat overview}
category_overview= data_s %>%
  group_by(data_channel) %>%
  summarise(avg_shares=mean(shares, na.rm = TRUE), 
            median_shares=median(shares),
            max_shares=max(shares),
            min_shares=min(shares),
            sd_shares=sd(shares),
            article_count=length(shares))
category_overview
```

The following section tries to provide a visual overview of how shares are distributed across channels.
```{r channel plot1}
qplot(data=data_s,
      factor(data_channel), 
      shares)
```

```{r channel plot2}
ggplot(data = data_s, 
        aes(x=factor(data_channel), 
            y = shares, 
            group = as.integer(data_channel))) +
  geom_point()+
  coord_cartesian(,c(0,50000)) + 
    geom_line(stat= "summary" ,
              fun.y = mean) 
```

```{r channel plot3}
#Introducing some colors
qplot(data=data_s, 
      factor(data_channel), 
      log(shares), 
      position="jitter", 
      colour=data_channel)
```

```{r channel plot4}
# check box plot to see if we can use ANOVA
qplot(data=data_s, 
      factor(data_channel), 
      shares, 
      geom="boxplot") + 
  coord_flip()

# To adjust for exponential scale, we took the logarthmic value of the # of shares
qplot(data=data_s, 
      factor(data_channel), 
      log(shares),
      geom="boxplot") + 
  coord_flip()
```


Possible differences in variances
H0: there are no differences in the number of shares between data_channels (topics)
Ha: there ARE differences in the number of shares between data_channels (topics)
```{r anova channels}
summary(aov(data_s$shares ~ data_s$data_channel))
```
The p-value is <2e-16, so with 95% confidence we can accept that there are statistically significant differences in number of shares between the different topics.

We will redo the ANOVA test without data_channel "Other" which has the largest variance
```{r channels !=other}
#REDO ANOVA WITHOUT DATA_CHANNEL=OTHER WHICH HAS THE LARGEST VARIANCE
noOther <- data_s %>%
  select(shares, data_channel) %>%
  filter(data_channel!="Other")

#verify the filter
count(noOther)/count(data_s)
```

Create a new boxplot
```{r channel plot 5}
qplot(data=noOther, 
      factor(data_channel), 
      log(shares), 
      geom="boxplot") + 
  coord_flip()
```


```{r anova channels !=other}
summary(aov(noOther$shares ~ noOther$data_channel))
```
ANOVA still has p-value < alpha which is statistically significant. We therefore reject H0.

WEEKDAY ANALYSIS
4. Looking at data at the weekday level

Basic statistical overview of shares per weekday:
```{r weekday overview}
weekday_overview = data_s %>%
  group_by(weekday) %>%
  summarise(avg_shares=mean(shares, na.rm = TRUE), 
            median_shares=median(shares),
            max_shares=max(shares),
            min_shares=min(shares),
            sd_shares=sd(shares),
            article_count=length(shares))
weekday_overview
```

Plotting how number of shares are distributed across weekdays.
```{r weekday plot1}
qplot(data=data_s, 
      factor(weekday),
      log(shares), 
      position="jitter", 
      colour=factor(weekday),
      main="Article shares per Weekday",
      xlab="Weekday",
      ylab="Log of Shares")
```
Creating box plots per weekday to see if we can use ANOVA
```{r weekday plot2}
# check box plot to see if we can use ANOVA
qplot(data=data_s, 
      factor(weekday), 
      shares,
      geom="boxplot") + 
  coord_flip()

# To adjust for exponential scale, we took the logarthmic value of the # of shares
qplot(data=data_s, 
      factor(weekday), 
      log(shares), 
      geom="boxplot") + 
  coord_flip()
```
It's clear that variances are fairly similar each day, implying it is okay to use ANOVA. At first glance, the means appear to be similar (bearing in mind that we've taken the log), but we will verify with an ANOVA test.

H0: there are no differences in the number of shares in any given day
Ha: there ARE differences in the number of shares in any given day

```{r weekday anova}
summary(aov(data_s$shares ~ data_s$weekday))
```
The p-value is 0.314, so we do not reject H0 that there are no statistically significant differences in number of shares between the days of the week.

Judging by the table above saturday and sundays seem to have higher means. There might be a difference in shares between weekends and weekdays? 1=Weekday, 0=Weekend

```{r weekday vs weekends}
weekend_overview= data_s %>%
  group_by(weekday_flag) %>%
  summarise(avg_shares=mean(shares, na.rm = TRUE), 
            median_shares=median(shares),
            max_shares=max(shares),
            min_shares=min(shares),
            sd_shares=sd(shares),
            article_count=length(shares))
weekend_overview
```

Coming up with a new hypothesis.
H0: there are no differences in the number of shares on weekdays vs weekends
Ha: there ARE differences in the number of shares on weekdays vs weekends.
```{r weekday plot3}
qplot(data=data_s,
      factor(weekday_flag),
      log(shares), 
      geom="boxplot") + 
  coord_flip()
```
Visually there does not seem to be significant difference. And check what ANOVA can tell us. (can we use ANOVA here for testing differences between only two groups or should be default to t-test?????)

```{r anova weekday flag}
summary(aov(data_s$shares ~ data_s$weekday_flag))
```
The p-value is 0.000734 and less than the significance level of 5% so we do reject H0 and have statistically significant evidence that the number of shares differ between weekdays and weekends.

Question: Which article was shared the most?
```{r}
#Article with the highest number of shares
data_s$url[which.max(data_s$shares)]
max(data_s$shares)
```

Question: Mean shares per group
```{r}
anova(data_s$data_channel, data_s$shares)
```

5. Regression Analysis:
Images
```{r images}
#Videos to shares
qplot(data=data_s[data_s$data_channel=="World",],
      num_imgs,
      shares,
      xlim=c(0, 25),
      ylim=c(0,5000),
      #colour=data_channel,
      xlab="Number of videos",
      ylab="Number of Shares",
      main="Relationship between Shares and videos") +
  geom_smooth(method="lm", se=FALSE) 
  

cor(data_s[data_s$data_channel=="World",]$shares, data_s$num_imgs)
lm


# build regression here?









# rename final model fit for regression diagnostics
fit <- 

# check for linearity - pattern in residuals?
qplot(predict(fit), resid(fit), geom="point") + geom_hline(yintercept=0)
# no clear patterns --> OK

# check for normality - 2 graphs (histogram vs. normal curve & sample-v-theoretical values)
qplot(rstandard(fit), geom="blank") + geom_histogram(aes(y=..density..), binwidth=0.5) + stat_function(fun=dnorm, args=list(mean=0,sd=1))
# looks normal --> OK

qplot(sample=rstandard(fit)) + geom_abline(slope=1, intercept=0)
# follows theoretical line more or less... a little  polynomial looking

# check for  homoschedacity
qplot(predict(fit), rstandard(fit), geom="point") + geom_hline(yintercept=0) +
geom_hline(yintercept=2, colour = I("red"), alpha=I(0.5)) +
geom_hline(yintercept=-2, colour = I("red"), alpha=I(0.5))


# check for independence via durbin
library(car)
durbinWatsonTest(fit)
#p-value=0 --> The residuals are not independent.



# test for outliers
outlierTest(fit)

# plot most influential variables
influencePlot(fit)




#Images

#Internal Href Links

#keywords in title

#token length (wordcount)

cor(data_s$weekday, data_s$shares)


#Days and cat into factors

```


```{r messing around}
summary(data_s)

#plotting images
ggplot(aes(y=shares, x=num_imgs), data=data_s) + 
  geom_point(alpha=1/20) +
  ylim(0, quantile(data_s$shares, probs=0.95)) +
  geom_smooth(method="lm", color="red")
  
  

```


```{r regression}
data_R = data_s %>%
  select(url,
         num_keywords,
         n_tokens_title,
         num_imgs,
         num_videos,
         n_tokens_content, 
         rate_positive_words,
         rate_negative_words,
         title_sentiment_polarity,
         title_subjectivity,
         title_sentiment_polarity,
         abs_title_subjectivity,
         abs_title_sentiment_polarity,
         average_token_length,
         shares,
         weekday)
#correlation between our variables
cor(data_R [,unlist(lapply(data_R, is.numeric))])

#it's clear that the variables have no big impact on the amount of shares (which is the logical target variable in this data set).
sub <- lm(data= data_R, abs_title_sentiment_polarity ~ title_sentiment_polarity + title_subjectivity + abs_title_subjectivity) 
summary(sub)

sub1 <- lm(data= data_R, abs_title_sentiment_polarity ~ (title_sentiment_polarity + title_subjectivity + abs_title_subjectivity)^ 2)
summary(sub1)

anova(sub,sub1)
  #sub1 is so far the best model

  #with interaction
sub2 <- lm(data= data_R, abs_title_sentiment_polarity ~ title_sentiment_polarity + (title_subjectivity + abs_title_subjectivity)^2)
summary(sub2)


sub3 <- lm(data= data_R, abs_title_sentiment_polarity ~ (title_sentiment_polarity + title_subjectivity)^2 + abs_title_subjectivity)
summary(sub3)

sub4 <- lm(data= data_R, abs_title_sentiment_polarity ~ title_subjectivity + (title_sentiment_polarity + abs_title_subjectivity)^2)
summary(sub4)

  #polynomial regression models
sub5 <- lm(data= data_R, abs_title_sentiment_polarity ~ ( I(title_sentiment_polarity^2) + I(title_subjectivity^2) + I(abs_title_subjectivity^2)))
summary(sub5)
anova(sub1,sub5)
  #sub5 is now the best model

sub6 <-  lm(data= data_R, abs_title_sentiment_polarity ~ ( I(title_sentiment_polarity^2) + (title_subjectivity) + (abs_title_subjectivity)))
summary(sub5)
anova(sub5,sub6)
  #we prefer sub6 since this model is less complex

sub7 <- lm(data= data_R, abs_title_sentiment_polarity ~ I(title_sentiment_polarity^2)) 
summary(sub7)
anova(sub6,sub7)
  #anova tells us to go for sub6 since it has significantly the highest R^2



#Checking linearity

qplot( predict(sub6), resid(sub6),geom= "point") + geom_hline(yintercept = 0)
    #there clearly seems to be a problem since there is a very clear pattern in the residuals.

#Checking Normality

q1= qplot(rstandard(sub6), geom="blank") + 
  geom_histogram(aes(y=..density..), colour=I("gray")) +
  stat_function(fun=dnorm, args=list(mean=0,sd=1), colour=I("red"), alpha=I(0.5))

q2 = qplot(sample=rstandard(sub6)) + geom_abline(slope=1, intercept = 0)
library(gridExtra)
grid.arrange(q1,q2,nrow=1)
#the graphs seem to imply that we cannot go out of a normal distribution of the residuals

# Checking Homoscedasticity
qplot(predict(sub6),resid(sub6), geom= "point") + geom_hline(y_intercept = 0)
spreadLevelPlot(sub6)
  #This doens't look like the homoscedasticity condition can be hold, the line should normally be horizontal and we can   see a clear change of variance in the residuals along the x-axis. 

# Checking Independence
durbinWatsonTest(sub6)
  # This test seems to be ok as the null-hypothesis of independence cannot be rejected.

# Checking Multicollinearity
vif(sub6)
  # This test shows no Multicollinearity, since all vifs are clearly smaller than 5.

# Outliers
outlierTest(sub6)


```


TECHNIQUES USED



Reducing Data:
1. __filter__    : To select some rows of the dataset
2. __select__    : To select columns
3. __arrange__   : To sort observations
4. __mutate__    : To modify the structure of the dataset (add variables, etc.)
5. __summarize__ : Together with group_by(), used to reduce variables to values




