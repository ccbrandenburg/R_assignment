---
title: "R Assignment"
output: html_document
---

Outline (to be deleted following completion):

1. Introduction to project
+ purpose
+ data
+ goal

2. Data exploration
+ description
+ explanation of data retained
+ data cleaning

3.Analyses
- Summary
- 1-D plots
- 2-D plots
- 3-D plots
- Correlations
- Regressions
- Validation

Summary
- results
- techniques used
- what was missing/failed
- conclusion (lessons learned)



INTRODUCTION

The purpose of this project was to explore a data set using R techniques learned during the course, particularly from the dplyr and ggplot2 libraries. The data set we selected contains information on nearly 40,000 articles published by Mashable over a 2-year period, detailing the day of publication, number of shares, topic of the content and keyword information. Our goal was to identify a meaningful relationship between the articles and their reach.


DATA CLEANING

The initial data set, downloaded from the University of California Irvine Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity), had 39644 observations and 61 variables.

Loading libraries
```{r library}
library(ggplot2)
library(dplyr)
```

Loading and examining the data
```{r load data and examine}
data=read.csv("OnlineNewsPopularity.csv", stringsAsFactors = FALSE)
glimpse(data)
dim(data)
tbl_df(data)
head(data)

```

To start, we selected only those columns that we believed had a higher probability of correlating positively with increased shares, such as type of content (images, links, videos), length of content (word count in title or article body), day of week published, topic of article, subjectivity and sentiment analysis information. The variables we removed either had unclear descriptions, were deemed redundant given other variables provided, or were not interesting enough to pursue in our analysis.

```{r variable-reduction}
data_s = data %>%
  select(url,
         timedelta,
         n_tokens_title,
         n_tokens_content,
  # removed variables 4-6
         num_hrefs,
         num_self_hrefs,
         num_imgs,
         num_videos,
         average_token_length,
         num_keywords,
         data_channel_is_lifestyle,
         data_channel_is_entertainment,
         data_channel_is_bus,
         data_channel_is_socmed,
         data_channel_is_tech,
         data_channel_is_world,
  # removed variables 19-30
         weekday_is_monday, 
         weekday_is_tuesday, 
         weekday_is_wednesday,
         weekday_is_thursday, 
         weekday_is_friday,
         weekday_is_saturday,
         weekday_is_sunday,
         is_weekend,
  # removed variables 39-43
         global_subjectivity,
         global_sentiment_polarity,
         global_rate_positive_words,
         global_rate_negative_words,
         rate_positive_words,
         rate_negative_words,
  # removed variables 50-55
         title_subjectivity,
         title_sentiment_polarity,
         abs_title_subjectivity,
         abs_title_sentiment_polarity,
         shares)
head(data_s)
```

To make the data easier to manage, we mutated the flag variables that could be easily consolidated into one categorical variable. For example, instead of a categorical variable citing which day of the week the article was published, the original data set had a flag variable for each day of the week stating whether or not the article had been published that day. This was also the case for data_channel, which describes the topic of the article.

There existed a flag variable called "is_weekend" that marks articles that were published on the weekend. We wanted to explore this, but the definition of weekend was not included. Rather than assume this meant Saturday and Sunday, we created our own flag variable called weekday, which represented any articles published Monday through Friday.

```{r mutating-variables}
data_s = data_s %>%
  #create weekday variable 
  mutate(weekday = ifelse(weekday_is_monday==1, 1,
                    ifelse(weekday_is_tuesday==1, 2,
                    ifelse(weekday_is_wednesday==1, 3,
                    ifelse(weekday_is_thursday==1, 4,
                    ifelse(weekday_is_friday==1, 5,
                    ifelse(weekday_is_saturday==1, 6,
                    ifelse(weekday_is_sunday==1, 7,0)))))))) %>%
  mutate(weekday_flag = ifelse(weekday<=5, 1, 0)) %>%
  #create channel variable
  mutate(data_channel = ifelse(data_channel_is_entertainment==1, "Entertainment",
                        ifelse(data_channel_is_bus==1, "Business",
                        ifelse(data_channel_is_socmed==1, "Social Media",
                        ifelse(data_channel_is_tech==1, "Tech",
                        ifelse(data_channel_is_world==1, "World",
                        ifelse(data_channel_is_lifestyle==1, "Lifestyle", "Other")))))))

summary(data_s)

```

DATA EXPLORATION

Our initial exploration of the data was focused on summarizing and visualizing the variables individually, and then plotting shares vs. various independent variables.


```{r}


# should we go in order here:
# 1. summarize the statistics & point out interesting tidbits
# 2. plot individual variables
# 3. plot individual variables vs. shares

qplot(data=data_s, timedelta, geom="histogram")

qplot(data=data_s,
      n_tokens_title,
      geom="histogram")

log.data = log

qplot(data=data_s,
      rate_positive_words,
      shares,
      log="xy") +
  geom_point(alpha=1/200)

ggplot (aes(x=rate_positive_words, y=shares), data=data_s, log="xy") +
    geom_point(alpha=1/20) +
    xlim(13,90)

cor(data_s$rate_positive_words, data_s$shares)
  
qplot(data=data_s,
      title_sentiment_polarity,
      geom="histogram")   

qplot(data=data_s,
      title_sentiment_polarity,
      shares)

summary(data_s$shares)


```

Trying out some stuff with graphs and overviews of weekday/categories
```{r}

qplot(data=data_s, x=factor(data_channel), y=1, size=shares, scale_y_discrete(0.5,1.5))



```







2. anything we can put before we deepdive into channels and weekdays?
``` {r stuff here}

#summary information



#playing around below
#number of articles per section
qplot(data=data_s,
      aes(x=reorder(data_channel,data_channel, function(x)-length(x))))+ geom_bar()


ggplot(theTable,
       aes(x=reorder(Position,Position,
                     function(x)-length(x)))) +
       geom_bar()
#trying to reorder things
ggplot(data_s, aes(x=reorder(data_channel, shares), y=shares, fill=reorder(data_channel, shares))) + geom_bar(stat=) + theme(axis.text.x=element_text(angle=45, vjust=0.5))


sum(data_s$weekday_flag)              
sum(data_s$weekday)

```

3. Looking at data at the channel level
``` {r channels}
#Avg Shares and articles published by category  <-- added this here as an overview
category_overview= data_s %>%
  group_by(data_channel) %>%
  summarise(avg_shares=mean(shares, na.rm = TRUE), 
            median_shares=median(shares),
            max_shares=max(shares),
            min_shares=min(shares),
            sd_shares=sd(shares),
            article_count=length(shares))

qplot(data=data_s,
      factor(data_channel), 
      shares)

#Introducing some colors
qplot(data=data_s, 
      factor(data_channel), 
      log(shares), 
      position="jitter", 
      colour=data_channel)

# check box plot to see if we can use ANOVA
qplot(data=data_s, 
      factor(data_channel), 
      shares, 
      geom="boxplot") + 
  coord_flip()

# To adjust for exponential scale, we took the logarthmic value of the # of shares
qplot(data=data_s, 
      factor(data_channel), 
      log(shares),
      geom="boxplot") + 
  coord_flip()

# COMMENT ABOUT POSSIBLE DIFFERENCES IN VARIANCES

# H0 is that there are no differences in the number of shares between data_channels (topics)
# Ha is that there ARE differences in the number of shares between data_channels (topics)

summary(aov(data_s$shares ~ data_s$data_channel))

# The p-value is <2e-16, so with 95% confidence we can accept that there are no statistically significant differences in number of shares between the different topics.


# REDO ANOVA WITHOUT DATA_CHANNEL=OTHER WHICH HAS THE LARGEST VARIANCE

noOther <- data_s %>%
  select(shares, data_channel) %>%
  filter(data_channel!="Other")

count(noOther)/count(data_s)

qplot(data=noOther, factor(data_channel), log(shares), geom="boxplot") + coord_flip()
# now they look good enough

summary(aov(noOther$shares ~ noOther$data_channel))

# anova still has p-value < alpha ---->>>>>>> statistically significant. Reject H0


# NEXT FIND THE BEST TOPIC FOR SHARING / OR / RANK THE TOPICS
```

4. Looking at data at the weekday level
``` {r weekday analysis}
# check box plot to see if we can use ANOVA --- WTF IS THIS? Still what is this??
qplot(data=data_s, factor(weekday), shares, geom="boxplot") + coord_flip() + scale_y_discrete(breaks=0:2000)

#Avg Shares and articles published by weekday <-- added this here as an overview
weekday_overview= data_s %>%
  group_by(weekday) %>%
  summarise(avg_shares=mean(shares, na.rm = TRUE), 
            median_shares=median(shares),
            max_shares=max(shares),
            min_shares=min(shares),
            sd_shares=sd(shares),
            article_count=length(shares))

#Plotting the all articles' shares per weekday
qplot(data=data_s, 
      factor(weekday),
      log(shares), 
      position="jitter", 
      colour=factor(weekday),
      main="Article shares per Weekday")
  

# check box plot to see if we can use ANOVA
qplot(data=data_s, 
      factor(weekday), 
      shares, 
      geom="boxplot") + 
  coord_flip()

# To adjust for exponential scale, we took the logarthmic value of the # of shares
qplot(data=data_s, 
      factor(weekday), 
      log(shares), 
      geom="boxplot") + 
  coord_flip()
# It's clear that variances are fairly similar each day, implying it is okay to use ANOVA. At first glance, the means appear to be similar (bearing in mind that we've taken the log), but we will verify with an ANOVA test

# H0: there are no differences in the number of shares in any given day
# Ha: there ARE differences in the number of shares in any given day
summary(aov(data_s$shares ~ data_s$weekday))

# The p-value is 0.314, so we do not reject H0 that there are no statistically significant differences in number of shares between the days of the week.
## if p< 5% (significance level) ->reject

# Judging by the table above saturday and sundays seem to have higher means. There might be a difference in shares between weekends and weekdays? 
weekend_overview= data_s %>%
  group_by(weekday_flag) %>%
  summarise(avg_shares=mean(shares, na.rm = TRUE), 
            median_shares=median(shares),
            max_shares=max(shares),
            min_shares=min(shares),
            sd_shares=sd(shares),
            article_count=length(shares))

# H0: there are no differences in the number of shares on weekdays vs weekends
# Ha: there ARE differences in the number of shares on weekdays vs weekends.

qplot(data=data_s,
      factor(weekday_flag),
      log(shares), 
      geom="boxplot") + 
  coord_flip()
#visually there does not seem to be a significant difference

summary(aov(data_s$shares ~ data_s$weekday_flag))
#The p-value is 0.000734 and less than the significance level of 5% so we do reject H0 and have statistically significant evidence that the number of shares differ between weekdays and weekends.
```

Question: Which article was shared the most?
```{r}
#Article with the highest number of shares
data_s$url[which.max(data_s$shares)]
#<<<<<<< HEAD
max(data_s$shares)
```

Question: Mean shares per group
```{r}
anova(data_s$data_channel, data_s$shares)
```

5. Regression Analysis:
Images
```{r images}
#Videos to shares
qplot(data=data_s[data_s$data_channel=="World",],
      num_imgs,
      shares,
      xlim=c(0, 25),
      ylim=c(0,5000),
      #colour=data_channel,
      xlab="Number of videos",
      ylab="Number of Shares",
      main="Relationship between Shares and videos") +
  geom_smooth(method="lm", se=FALSE) 
  

cor(data_s[data_s$data_channel=="World",]$shares, data_s$num_imgs)
lm


# build regression here?









# rename final model fit for regression diagnostics
fit <- 

# check for linearity - pattern in residuals?
qplot(predict(fit), resid(fit), geom="point") + geom_hline(yintercept=0)
# no clear patterns --> OK

# check for normality - 2 graphs (histogram vs. normal curve & sample-v-theoretical values)
qplot(rstandard(fit), geom="blank") + geom_histogram(aes(y=..density..), binwidth=0.5) + stat_function(fun=dnorm, args=list(mean=0,sd=1))
# looks normal --> OK

qplot(sample=rstandard(fit)) + geom_abline(slope=1, intercept=0)
# follows theoretical line more or less... a little  polynomial looking

# check for  homoschedacity
qplot(predict(fit), rstandard(fit), geom="point") + geom_hline(yintercept=0) +
geom_hline(yintercept=2, colour = I("red"), alpha=I(0.5)) +
geom_hline(yintercept=-2, colour = I("red"), alpha=I(0.5))


# check for independence via durbin
library(car)
durbinWatsonTest(fit)
#p-value=0 --> The residuals are not independent.



# test for outliers
outlierTest(fit)

# plot most influential variables
influencePlot(fit)




#Images

#Internal Href Links

#keywords in title

#token length (wordcount)

cor(data_s$weekday, data_s$shares)


#Days and cat into factors

```


```{r messing around}
summary(data_s)

#plotting images
ggplot(aes(y=shares, x=num_imgs), data=data_s) + 
  geom_point(alpha=1/20) +
  ylim(0, quantile(data_s$shares, probs=0.95)) +
  geom_smooth(method="lm", color="red")
  
  

```


```{r regression}
data_R = data %>%
  select(url,
         num_keywords,
         n_tokens_title,
         num_imgs,
         num_videos,
         n_tokens_content, 
         rate_positive_words,
         rate_negative_words,
         title_sentiment_polarity,
         title_subjectivity,
         title_sentiment_polarity,
         abs_title_subjectivity,
         abs_title_sentiment_polarity,
         average_token_length,
         shares,
         weekday)
#correlation between our variables
cor(data_R [,unlist(lapply(data_R, is.numeric))])

#it's clear that the variables have no big impact on the amount of shares (which is the logical target variable in this data set).
sub <- lm(data= data_R, abs_title_sentiment_polarity ~ title_sentiment_polarity + title_subjectivity + abs_title_subjectivity) 
summary(sub)

sub1 <- lm(data= data_R, abs_title_sentiment_polarity ~ (title_sentiment_polarity + title_subjectivity + abs_title_subjectivity)^ 2)
summary(sub1)

anova(sub,sub1)
  #sub1 is so far the best model

  #with interaction
sub2 <- lm(data= data_R, abs_title_sentiment_polarity ~ title_sentiment_polarity + (title_subjectivity + abs_title_subjectivity)^2)
summary(sub2)


sub3 <- lm(data= data_R, abs_title_sentiment_polarity ~ (title_sentiment_polarity + title_subjectivity)^2 + abs_title_subjectivity)
summary(sub3)

sub4 <- lm(data= data_R, abs_title_sentiment_polarity ~ title_subjectivity + (title_sentiment_polarity + abs_title_subjectivity)^2)
summary(sub4)

  #polynomial regression models
sub5 <- lm(data= data_R, abs_title_sentiment_polarity ~ ( I(title_sentiment_polarity^2) + I(title_subjectivity^2) + I(abs_title_subjectivity^2)))
summary(sub5)
anova(sub1,sub5)
  #sub5 is now the best model

sub6 <-  lm(data= data_R, abs_title_sentiment_polarity ~ ( I(title_sentiment_polarity^2) + (title_subjectivity) + (abs_title_subjectivity)))
summary(sub5)
anova(sub5,sub6)
  #we prefer sub6 since this model is less complex

sub7 <- lm(data= data_R, abs_title_sentiment_polarity ~ I(title_sentiment_polarity^2)) 
summary(sub7)
anova(sub6,sub7)
  #anova tells us to go for sub6 since it has significantly the highest R^2



#checking linearity

qplot( predict(sub6), resid(sub6),geom= "point") + geom_hline(yintercept = 0)
```


TECHNIQUES USED



Reducing Data:
1. __filter__    : To select some rows of the dataset
2. __select__    : To select columns
3. __arrange__   : To sort observations
4. __mutate__    : To modify the structure of the dataset (add variables, etc.)
5. __summarize__ : Together with group_by(), used to reduce variables to values




